{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNBClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Smoothing parameter\n",
    "        self.class_priors = {}\n",
    "        self.feature_probs = {}\n",
    "        self.classes = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Calculate class priors and feature likelihoods\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[c] = len(X_c) / len(X)\n",
    "            \n",
    "            # Count the occurrences of each feature in the class\n",
    "            feature_counts = X_c.sum(axis=0)\n",
    "            total_count = feature_counts.sum()\n",
    "\n",
    "            # Calculate likelihood with Laplace smoothing\n",
    "            self.feature_probs[c] = (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
    "\n",
    "    def _calculate_posterior(self, x):\n",
    "        posteriors = {}\n",
    "        for c in self.classes:\n",
    "            # Log of the prior for the class\n",
    "            prior = np.log(self.class_priors[c])\n",
    "            # Sum log-likelihoods of features\n",
    "            likelihood = np.sum(x * np.log(self.feature_probs[c]))\n",
    "            posteriors[c] = prior + likelihood\n",
    "        return posteriors\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            # Select class with highest posterior\n",
    "            y_pred.append(max(posteriors, key=posteriors.get))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        # Return the accuracy score\n",
    "        return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('full_cleaned_data.csv')\n",
    "data_frame = data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for entry in data_frame:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "texts = data_frame['text']\n",
    "labels = data_frame['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       neutral\n",
       "1      negative\n",
       "2      positive\n",
       "3      negative\n",
       "4      negative\n",
       "         ...   \n",
       "251     neutral\n",
       "252     neutral\n",
       "253     neutral\n",
       "254     neutral\n",
       "255     neutral\n",
       "Name: label, Length: 256, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ubah(x):\n",
    "    if x == 'neutral':\n",
    "        return 1\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    if x == 'positive':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.apply(ubah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      2\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "251    1\n",
       "252    1\n",
       "253    1\n",
       "254    1\n",
       "255    1\n",
       "Name: label, Length: 256, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert text to numerical features\n",
    "countvector = CountVectorizer()\n",
    "X = countvector.fit_transform(texts)\n",
    "X = X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204, 1555), (204,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143    1\n",
       "84     1\n",
       "55     1\n",
       "220    1\n",
       "104    1\n",
       "      ..\n",
       "106    0\n",
       "14     1\n",
       "92     1\n",
       "179    1\n",
       "102    0\n",
       "Name: label, Length: 204, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train a Naive Bayes classifier\n",
    "classifier = MultinomialNBClassifier()\n",
    "k_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, val_index in k_folds.split(X_train_np, y_train_np):\n",
    "    X_train_fold, X_val_fold = X_train_np[train_index], X_train_np[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_np[train_index], y_train_np[val_index]\n",
    "\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    accuracy = classifier.score(X_val_fold, y_val_fold)\n",
    "    scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.4878048780487805, 0.4146341463414634, 0.7317073170731707, 0.3902439024390244, 0.625]\n",
      "Average accuracy:  0.5298780487804878\n"
     ]
    }
   ],
   "source": [
    "average_accuracy = np.mean(scores)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Average accuracy: ', average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.4878048780487805, 0.4146341463414634, 0.7317073170731707, 0.3902439024390244, 0.625]\n",
      "Average CV Score:  0.5298780487804878\n",
      "Number of CV Scores used in Average:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross Validation Score: \", scores)\n",
    "print(\"Average CV Score: \", np.mean(scores))\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Make predictions and evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.to_numpy().reshape((52,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52,), (52,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y_pred  y_test\n",
      "0        1       1\n",
      "1        0       1\n",
      "2        1       1\n",
      "3        2       1\n",
      "4        1       2\n",
      "5        1       1\n",
      "6        1       1\n",
      "7        0       0\n",
      "8        1       0\n",
      "9        1       0\n",
      "10       0       1\n",
      "11       1       1\n",
      "12       1       2\n",
      "13       1       1\n",
      "14       1       1\n",
      "15       1       1\n",
      "16       1       1\n",
      "17       2       0\n",
      "18       0       0\n",
      "19       1       1\n",
      "20       2       2\n",
      "21       1       1\n",
      "22       0       1\n",
      "23       1       1\n",
      "24       1       1\n",
      "25       0       1\n",
      "26       1       1\n",
      "27       0       0\n",
      "28       1       1\n",
      "29       1       1\n",
      "30       1       2\n",
      "31       0       0\n",
      "32       1       0\n",
      "33       0       0\n",
      "34       1       1\n",
      "35       1       1\n",
      "36       0       1\n",
      "37       1       0\n",
      "38       1       1\n",
      "39       1       2\n",
      "40       0       0\n",
      "41       0       0\n",
      "42       1       1\n",
      "43       0       0\n",
      "44       1       1\n",
      "45       2       2\n",
      "46       1       1\n",
      "47       1       0\n",
      "48       0       2\n",
      "49       1       1\n",
      "50       1       2\n",
      "51       0       1\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'y_pred': y_pred, 'y_test': y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55        15\n",
      "           1       0.77      0.70      0.73        33\n",
      "           2       0.25      0.50      0.33         4\n",
      "\n",
      "    accuracy                           0.63        52\n",
      "   macro avg       0.53      0.58      0.54        52\n",
      "weighted avg       0.67      0.63      0.65        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
