{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNBClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Smoothing parameter\n",
    "        self.class_priors = {}\n",
    "        self.feature_probs = {}\n",
    "        self.classes = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Calculate class priors and feature likelihoods\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[c] = len(X_c) / len(X)\n",
    "            \n",
    "            # Count the occurrences of each feature in the class\n",
    "            feature_counts = X_c.sum(axis=0)\n",
    "            total_count = feature_counts.sum()\n",
    "\n",
    "            # Calculate likelihood with Laplace smoothing\n",
    "            self.feature_probs[c] = (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
    "\n",
    "    def _calculate_posterior(self, x):\n",
    "        posteriors = {}\n",
    "        for c in self.classes:\n",
    "            # Log of the prior for the class\n",
    "            prior = np.log(self.class_priors[c])\n",
    "            # Sum log-likelihoods of features\n",
    "            likelihood = np.sum(x * np.log(self.feature_probs[c]))\n",
    "            posteriors[c] = prior + likelihood\n",
    "        return posteriors\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            # Select class with highest posterior\n",
    "            y_pred.append(max(posteriors, key=posteriors.get))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        # Return the accuracy score\n",
    "        return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('data/full_cleaned_data.csv')\n",
    "data_frame = data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for entry in data_frame:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "texts = data_frame['text']\n",
    "labels = data_frame['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      positif\n",
       "1      negatif\n",
       "2      negatif\n",
       "3       netral\n",
       "4      positif\n",
       "        ...   \n",
       "372     netral\n",
       "373     netral\n",
       "374    negatif\n",
       "375     netral\n",
       "376     netral\n",
       "Name: label, Length: 377, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ubah(x):\n",
    "    if x == 'netral':\n",
    "        return 1\n",
    "    if x == 'negatif':\n",
    "        return 0\n",
    "    if x == 'positif':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.apply(ubah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    151\n",
       "0    124\n",
       "2    102\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert text to numerical features\n",
    "countvector = CountVectorizer()\n",
    "X = countvector.fit_transform(texts)\n",
    "X = X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((301, 2149), (301,))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137    0\n",
       "101    0\n",
       "354    1\n",
       "196    1\n",
       "222    2\n",
       "      ..\n",
       "71     0\n",
       "106    2\n",
       "270    1\n",
       "348    0\n",
       "102    0\n",
       "Name: label, Length: 301, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train a Naive Bayes classifier\n",
    "classifier = MultinomialNBClassifier()\n",
    "k_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, val_index in k_folds.split(X_train_np, y_train_np):\n",
    "    X_train_fold, X_val_fold = X_train_np[train_index], X_train_np[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_np[train_index], y_train_np[val_index]\n",
    "\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    accuracy = classifier.score(X_val_fold, y_val_fold)\n",
    "    scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.5245901639344263, 0.4, 0.5333333333333333, 0.45, 0.48333333333333334]\n",
      "Average accuracy:  0.4782513661202185\n"
     ]
    }
   ],
   "source": [
    "average_accuracy = np.mean(scores)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Average accuracy: ', average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.5245901639344263, 0.4, 0.5333333333333333, 0.45, 0.48333333333333334]\n",
      "Average CV Score:  0.4782513661202185\n",
      "Number of CV Scores used in Average:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross Validation Score: \", scores)\n",
    "print(\"Average CV Score: \", np.mean(scores))\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Make predictions and evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.to_numpy().reshape((76,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76,), (76,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y_pred  y_test\n",
      "0        0       0\n",
      "1        0       1\n",
      "2        0       1\n",
      "3        2       0\n",
      "4        0       0\n",
      "..     ...     ...\n",
      "71       0       2\n",
      "72       0       1\n",
      "73       1       0\n",
      "74       1       1\n",
      "75       1       1\n",
      "\n",
      "[76 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'y_pred': y_pred, 'y_test': y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.52      0.57        29\n",
      "           1       0.50      0.47      0.48        34\n",
      "           2       0.20      0.31      0.24        13\n",
      "\n",
      "    accuracy                           0.46        76\n",
      "   macro avg       0.44      0.43      0.43        76\n",
      "weighted avg       0.50      0.46      0.47        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
