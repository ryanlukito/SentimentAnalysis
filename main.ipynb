{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNBClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Smoothing parameter\n",
    "        self.class_priors = {}\n",
    "        self.feature_probs = {}\n",
    "        self.classes = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Calculate class priors and feature likelihoods\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[c] = len(X_c) / len(X)\n",
    "            \n",
    "            # Count the occurrences of each feature in the class\n",
    "            feature_counts = X_c.sum(axis=0)\n",
    "            total_count = feature_counts.sum()\n",
    "\n",
    "            # Calculate likelihood with Laplace smoothing\n",
    "            self.feature_probs[c] = (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
    "\n",
    "    def _calculate_posterior(self, x):\n",
    "        posteriors = {}\n",
    "        for c in self.classes:\n",
    "            # Log of the prior for the class\n",
    "            prior = np.log(self.class_priors[c])\n",
    "            # Sum log-likelihoods of features\n",
    "            likelihood = np.sum(x * np.log(self.feature_probs[c]))\n",
    "            posteriors[c] = prior + likelihood\n",
    "        return posteriors\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            # Select class with highest posterior\n",
    "            y_pred.append(max(posteriors, key=posteriors.get))\n",
    "        return np.array(y_pred)\n",
    "    def score(self, X, y):\n",
    "        # Predict labels for the input data\n",
    "        y_pred = self.predict(X)\n",
    "        # Calculate and return the accuracy\n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('full_cleaned_data.csv')\n",
    "data_frame = data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for entry in data_frame:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "texts = data_frame['text']\n",
    "labels = data_frame['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       neutral\n",
       "1      negative\n",
       "2      positive\n",
       "3      negative\n",
       "4      negative\n",
       "         ...   \n",
       "251     neutral\n",
       "252     neutral\n",
       "253     neutral\n",
       "254     neutral\n",
       "255     neutral\n",
       "Name: label, Length: 256, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ubah(x):\n",
    "    if x == 'neutral':\n",
    "        return 1\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    if x == 'positive':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.apply(ubah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      2\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "251    1\n",
       "252    1\n",
       "253    1\n",
       "254    1\n",
       "255    1\n",
       "Name: label, Length: 256, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for current fold: 0.6346153846153846\n",
      "Score for current fold: 0.6470588235294118\n",
      "Score for current fold: 0.5686274509803921\n",
      "Score for current fold: 0.49019607843137253\n",
      "Score for current fold: 0.5098039215686274\n",
      "Best score across all folds: 0.6470588235294118\n",
      "Final evaluation on the best fold: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Convert text to numerical features\n",
    "countvector = CountVectorizer()\n",
    "X = countvector.fit_transform(texts)\n",
    "X = X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model = MultinomialNBClassifier()\n",
    "\n",
    "best_score = 0  # Initialize variable to track the best score\n",
    "best_train_index, best_test_index = None, None  # To store the best fold indices\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Fit the model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"Score for current fold:\", score)\n",
    "\n",
    "    # Update best_score and store indices if the current score is higher\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_train_index, best_test_index = train_index, test_index\n",
    "\n",
    "# Re-fit the model on the best fold\n",
    "X_train_best, X_test_best = X[best_train_index], X[best_test_index]\n",
    "y_train_best, y_test_best = labels[best_train_index], labels[best_test_index]\n",
    "model.fit(X_train_best, y_train_best)\n",
    "\n",
    "print(\"Best score across all folds:\", best_score)\n",
    "print(\"Final evaluation on the best fold:\", model.score(X_test_best, y_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((205, 1555), (205,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_best.shape, y_train_best.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train a Naive Bayes classifier\n",
    "model = MultinomialNBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.to_numpy().reshape((51,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51,), (51,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y_pred  y_test\n",
      "0        0       0\n",
      "1        1       1\n",
      "2        1       1\n",
      "3        0       0\n",
      "4        1       1\n",
      "5        1       1\n",
      "6        0       0\n",
      "7        1       1\n",
      "8        0       0\n",
      "9        0       0\n",
      "10       1       1\n",
      "11       1       1\n",
      "12       1       1\n",
      "13       2       2\n",
      "14       0       0\n",
      "15       0       0\n",
      "16       1       1\n",
      "17       0       0\n",
      "18       0       0\n",
      "19       0       0\n",
      "20       1       0\n",
      "21       0       0\n",
      "22       2       2\n",
      "23       1       1\n",
      "24       0       0\n",
      "25       0       0\n",
      "26       1       1\n",
      "27       0       0\n",
      "28       0       0\n",
      "29       1       1\n",
      "30       0       0\n",
      "31       2       2\n",
      "32       1       1\n",
      "33       0       0\n",
      "34       1       1\n",
      "35       1       1\n",
      "36       1       1\n",
      "37       0       0\n",
      "38       2       2\n",
      "39       2       2\n",
      "40       2       2\n",
      "41       1       1\n",
      "42       1       1\n",
      "43       1       1\n",
      "44       0       0\n",
      "45       2       2\n",
      "46       0       0\n",
      "47       0       0\n",
      "48       1       1\n",
      "49       1       1\n",
      "50       1       1\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'y_pred': y_pred, 'y_test': y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        21\n",
      "           1       1.00      0.96      0.98        23\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.98        51\n",
      "   macro avg       0.98      0.99      0.98        51\n",
      "weighted avg       0.98      0.98      0.98        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
